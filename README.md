# spark_training_app

We are making a Python training application that consists of four parts:

  Part 1 - Data Load:
  - Connects to the DB2 instance on the cloud
  - Generates a table with 20k of sample records
    - product_id - autogenerated numeric
    - product_group - autogenerated numeric in 0..9 range
    - year - autogenerated numeric in 2015..2018 range
    - 12 columns with monthly purchases amount - numeric in 0..100000 range
  - There should be no duplicates for the same product/year
  - If there are multiple product rows for different years, product/group combination should be concise



  Part 2 - Data Transformation:
  - Create a data transformation application using Apache Spark
  - Read data from the DB2 on the cloud (use the service instance and data from the previous task)
  - Aggregate data in the dataframe by calculating total purchases amount per year
  - For each row calculate total of monthly purchases
  - Save the year total as a new column year_purchases
  - Remove the columns with monthly amounts from the data frame
  - Save the modified dataframe as a file in Cloud Object Storage - https://www.ibm.com/cloud/object-storage
  - Use spark-submit to run the application and Spark configuration properties to specify configuration parameters like DB URL and connection credentials



  Part 3 - Unit tests:
  - Add tests into the application
  
  
  Part 4 - Spark + Kubernetes:
  - Run the application on minikube


How to use it:
- Create an instance of DB2 service on the cloud - [DB2 instruction](https://cloud.ibm.com/docs/Db2onCloud?topic=Db2onCloud-faq_db2oc)
- Create an instance of IBM Cloud Object Storage -  [COS instruction](https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-getting-started-cloud-object-storage)
- Use environment variables to specify credentials - [use command SETX](https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/setx#:~:text=The%20Setx%20command%20also%20retrieves,or%20through%20a%20registry%20editor.)
  - You have to set following environment variables (use service credentials' data - they're different for DB2 and  COS accounts!):
    - DB2_USER 
    - DB2_PASSWORD
    - DB2_HOST
- Install and configure PyCharm. + SQLAlchemy, ibm-db, pyspark, spark modules and customize spark on your computer - [instruction for Windows and MacOS](https://medium.com/cluj-school-of-ai/getting-started-with-pycharm-d9f58467017)
- Install Stocator - [Instruction](https://github.com/CODAIT/stocator)
  - Copy stocator-X-Y-Z-jar-with-dependencies.jar to SPARK_HOME\jars folder
- Install and configure Docker, Kubernetes and Minikube.
- Clone this repo and open the project in PyCharm. Configure Spark in the PyCharm project.

- Run data_load.py:
  
  - Configure Run of PyCharm.
  - You have to get following output: '20000 rows was successfully created'

- Run data_transformation.py using spark-submit:
  - Open bash
  - Set current folder to SPARK_HOME
  - Run:  `.bin/spark-submit  --master local[*]  ./path/to/execute/file/data_transformation.py`
  - Check your COS bucket (you need to remove freshly added files)

- Run data_transformation_test.py using PyCharm:
  - Configure Run of PyCharm.
  - If the test is successful, you will receive no output.
    
- Run data_transformation_test.py using Minikube:
  - Add folder 'app' to the SPARK_HOME folder
  - Put the 'data_transformation.py' file in this folder
  - Open bash
  - Set current folder to SPARK_HOME
  - Change Dockerfile from ./kubernetes/dockerfiles/spark to the Dockerfile from the repo
    - You have to define the following credentials in the dockerfile:
        - DB2_USER 
        - DB2_PASSWORD
        - DB2_URL
        - DB2_TABLE_NAME
        - COS_ENDPOINT
        - COS_ACCESS_KEY
        - COS_SECRET_KEY
        - COS_BUCKET_NAME
        - COS_DF_NAME
  - Enter commands:
    
    `minikube start` 
    
    `kubectl cluster-info`
  
    `./bin/docker-image-tool.sh -r {your_docker_name} -p ./kubernetes/dockerfiles/spark/bindings/python/Dockerfile -t latest build`
    
     `./bin/docker-image-tool.sh -r {your_docker_name} -p ./kubernetes/dockerfiles/spark/bindings/python/Dockerfile -t latest push`
    
  - Run: 
    
    `.bin/spark-submit \`
    
     `--master k8s://{kubectl cluster}  \`
  
      `--deploy-mode cluster \`

      `--name spark-pi \`

      `--conf spark.executor.instances=3 \`
  
    `--conf spark.kubernetes.container.image={image_name}:latest \`

    `--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark  \`

    `local:///opt/spark/app/data_transformation.py`
  - Check your COS bucket (you need to remove freshly added files)