# spark_training_app

First of all:
- Create an instance of DB2 service on the cloud - [instruction](https://cloud.ibm.com/docs/Db2onCloud?topic=Db2onCloud-faq_db2oc)
- Use environment variables to specify (USER_NAME, PASSWORD and HOST) - [use comand SETX](https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/setx#:~:text=The%20Setx%20command%20also%20retrieves,or%20through%20a%20registry%20editor.)
- Install and configure PyCharm. + SQLAlchemy, ibm-db, pyspark, spark modules and customize spark on your computer - [instruction for Windows and MacOS](https://medium.com/cluj-school-of-ai/getting-started-with-pycharm-d9f58467017)


We are making a Python training application that consists of four parts:

Part 1 - Data Load:
- Connects to the DB2 instance on the cloud
- Generates a table with 20k of sample records
  - product_id - autogenerated numeric
  - product_group - autogenerated numeric in 0..9 range
  - year - autogenerated numeric in 2015..2018 range
  - 12 columns with monthly purchases amount - numeric in 0..100000 range
- There should be no duplicates for the same product/year
- If there are multiple product rows for different years, product/group combination should be concise

Part 2 - Data Transformation:
- Create a data tranformation application using Apache Spark
- Read data from the DB2 on the cloud (use the service instance and data from the previous task)
- Aggregate data in the dataframe by calculating total purchases amount per year
- For each row calculate total of monthly purchases
- Save the year total as a new column year_purchases
- Remove the columns with monthly amounts from the data frame
- Save the modified dataframe as a file in Cloud Object Storage - https://www.ibm.com/cloud/object-storage
- Use spark-submit to run the application and Spark configuration properties to specify configuration parameters like DB URL and connection credentials
- Submitting Applications - https://spark.apache.org/docs/latest/submitting-applications.html
- Method to get Spark configuration properties - https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/SparkContext.html#getConf--

Part 3 - Unit tests:
- Add tests into your application


Part 4 - Spark + Kubernetes:
- Run your application on minikube
